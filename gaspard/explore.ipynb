{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cfb1c77",
   "metadata": {},
   "source": [
    "# Cenaero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b6aaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 6)\n",
    "\n",
    "os.makedirs('../outputs', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5c3830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "NUM_SEQUENCES = 121\n",
    "DATA_PATH = '../data/38Q31TzlO-{}/npz_data/data.npz'\n",
    "PARAMS_PATH = '../data/38Q31TzlO-{}/Minamo_Parameters-Wall2D.txt'\n",
    "RANDOM_SEED = 20210831"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8862cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and parsing methods\n",
    "\n",
    "def load_data(simulation_ids, recurrent=False):\n",
    "    \n",
    "    inputs, targets = [], []\n",
    "    \n",
    "    for simulation_id in simulation_ids:\n",
    "\n",
    "        data = np.load(DATA_PATH.format(simulation_id))\n",
    "\n",
    "        # Unused\n",
    "        # T_top = data['T_top']\n",
    "        # x = data['x']\n",
    "        # y = data['y']\n",
    "        # temperatures = data['temperatures']\n",
    "        \n",
    "        # Parse parameters\n",
    "        with open(PARAMS_PATH.format(simulation_id)) as params_file:\n",
    "            lines = params_file.read().splitlines()\n",
    "            power = float(lines[0].split(' = ')[1])\n",
    "            break_time = float(lines[1].split(' = ')[1])\n",
    "\n",
    "        # Input data\n",
    "        time = data['time']\n",
    "        delta = time.copy()\n",
    "        delta[1:] = time[1:] - time[:-1]\n",
    "        laser_position = data['laser_position_x']\n",
    "        laser_power = data['laser_power']\n",
    "        power = np.full(laser_power.shape, power)\n",
    "        break_time = np.full(laser_power.shape, break_time)\n",
    "        if not recurrent:\n",
    "            input = np.stack([time, laser_position, laser_power, power, break_time], axis=1)\n",
    "        else:\n",
    "            input = np.stack([delta, laser_position, laser_power, power, break_time], axis=1)\n",
    "\n",
    "        # Target data\n",
    "        target = np.stack([data['T{}'.format(i + 1)] for i in range(6)], axis=1)\n",
    "\n",
    "        inputs.append(input)\n",
    "        targets.append(target)\n",
    "    \n",
    "    if not recurrent:\n",
    "        inputs = np.concatenate(inputs, axis=0)\n",
    "        targets = np.concatenate(targets, axis=0)\n",
    "        \n",
    "    else:\n",
    "        max_len = max(input.shape[0] for input in inputs)\n",
    "        \n",
    "        for i, input in enumerate(inputs):\n",
    "            inputs[i] = np.pad(input, [(0, max_len - input.shape[0]), (0, 0)])\n",
    "        for i, target in enumerate(targets):\n",
    "            targets[i] = np.pad(target, [(0, max_len - target.shape[0]), (0, 0)])\n",
    "        \n",
    "        inputs = np.stack(inputs, axis=1)\n",
    "        targets = np.stack(targets, axis=1)\n",
    "    \n",
    "    return inputs.astype(np.float32), targets.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44995fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview of the data\n",
    "\n",
    "inputs, targets = load_data(range(1, 8 + 1), recurrent=True)\n",
    "\n",
    "print('inputs:', inputs.shape)\n",
    "print('targets:', targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedd102d",
   "metadata": {},
   "source": [
    "## Train, test and validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bcc72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "permutation = np.random.permutation(np.arange(1, NUM_SEQUENCES + 1))\n",
    "first_split = int(0.7 * NUM_SEQUENCES)\n",
    "second_split = int(0.85 * NUM_SEQUENCES)\n",
    "train_sequence_ids = permutation[:first_split]\n",
    "valid_sequence_ids = permutation[first_split:second_split]\n",
    "test_sequence_ids = permutation[second_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3588ac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, train_targets = load_data(train_sequence_ids, recurrent=False)\n",
    "valid_inputs, valid_targets = load_data(valid_sequence_ids, recurrent=False)\n",
    "test_inputs, test_targets = load_data(test_sequence_ids, recurrent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6ce56a",
   "metadata": {},
   "source": [
    "## Machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdced8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=200, max_depth=None, random_state=RANDOM_SEED, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6998c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(train_inputs, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac1e1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_preds = rf.predict(valid_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6d91c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mse_loss = ((valid_preds - valid_targets) ** 2).sum() / valid_preds.shape[0]\n",
    "print(mse_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646235f7",
   "metadata": {},
   "source": [
    "### Validation sample sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fd9df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_valid_id = np.random.choice(valid_sequence_ids, size=(1,))\n",
    "sample_valid_inputs, sample_valid_targets = load_data(sample_valid_id, recurrent=False)\n",
    "\n",
    "sample_valid_preds = rf.predict(sample_valid_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f74001",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    plt.plot(sample_valid_targets[:, i], color='C{}'.format(i), label='Target {}'.format(i))\n",
    "    plt.plot(sample_valid_preds[:, i], color='C{}'.format(i), linestyle=':', label='Prediction {}'.format(i))\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10275197",
   "metadata": {},
   "source": [
    "### Training sample sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ba0d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_id = np.random.choice(train_sequence_ids, size=(1,))\n",
    "sample_train_inputs, sample_train_targets = load_data(sample_train_id, recurrent=False)\n",
    "\n",
    "sample_train_preds = rf.predict(sample_train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5927467a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    plt.plot(sample_train_targets[:, i], color='C{}'.format(i), label='Target {}'.format(i))\n",
    "    plt.plot(sample_train_preds[:, i], color='C{}'.format(i), linestyle=':', label='Prediction {}'.format(i))\n",
    "    plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c91e223",
   "metadata": {},
   "source": [
    "## Deep learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08411669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb986c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_hidden):\n",
    "        super().__init__()\n",
    "        \n",
    "        hidden_layers = []\n",
    "        for _ in range(num_hidden - 1):\n",
    "            hidden_layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            hidden_layers.append(nn.ReLU())\n",
    "        \n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            *hidden_layers,\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.sequential(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a009299f",
   "metadata": {},
   "source": [
    "### Convert data array to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c81f208",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = torch.from_numpy(train_inputs)\n",
    "train_targets = torch.from_numpy(train_targets)\n",
    "valid_inputs = torch.from_numpy(valid_inputs)\n",
    "valid_targets = torch.from_numpy(valid_targets)\n",
    "test_inputs = torch.from_numpy(test_inputs)\n",
    "test_targets = torch.from_numpy(test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8840ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = train_inputs.size(0)\n",
    "num_valid = valid_inputs.size(0)\n",
    "num_test = test_inputs.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd81c394",
   "metadata": {},
   "source": [
    "### Hyperparameters and instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93052e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_HIDDEN = 2\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCH_CONVERGENCE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860006ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(\n",
    "    input_size=train_inputs.size(1),\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    output_size=train_targets.size(1),\n",
    "    num_hidden=NUM_HIDDEN)\n",
    "opt = optim.Adam(mlp.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "model_name = 'mlp-' + '-'.join(str(HIDDEN_SIZE) for _ in range(NUM_HIDDEN))\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96683e8",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c43a229",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "lowest_loss, num_epoch_no_improvement = float('inf'), 0\n",
    "best_weights = deepcopy(mlp.state_dict())\n",
    "train_losses, valid_losses = [], []\n",
    "train_after_epoch_losses = []  # TODO: delete this\n",
    "\n",
    "epoch = 0\n",
    "while num_epoch_no_improvement <= NUM_EPOCH_CONVERGENCE:\n",
    "    \n",
    "    # Training\n",
    "    permutation = torch.randperm(num_train)\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for i in range(0, num_train, BATCH_SIZE):\n",
    "        indices = permutation[i:i+BATCH_SIZE]\n",
    "        batch_inputs = train_inputs[indices, :]\n",
    "        batch_targets = train_targets[indices, :]\n",
    "        \n",
    "        batch_preds = mlp(batch_inputs)\n",
    "        loss = F.mse_loss(batch_preds, batch_targets)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= int(num_train / BATCH_SIZE)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    epoch += 1\n",
    "    \n",
    "    # Training evaluation after epoch\n",
    "    with torch.no_grad():\n",
    "        train_preds = mlp(train_inputs)\n",
    "        train_after_epoch_loss = F.mse_loss(train_preds, train_targets).item()\n",
    "    \n",
    "    train_after_epoch_losses.append(train_after_epoch_loss)\n",
    "    \n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        valid_preds = mlp(valid_inputs)\n",
    "        valid_loss = F.mse_loss(valid_preds, valid_targets).item()\n",
    "    \n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    if valid_loss < lowest_loss:\n",
    "        lowest_loss = valid_loss\n",
    "        num_epoch_no_improvement = 0\n",
    "        best_weights = deepcopy(mlp.state_dict())\n",
    "    else:\n",
    "        num_epoch_no_improvement += 1\n",
    "    \n",
    "    print('Epoch {:03d}: train: {:.4f}, train after epoch: {:.4f}, valid: {:.4f}'.format(epoch, train_loss, train_after_epoch_loss, valid_loss))\n",
    "\n",
    "mlp.load_state_dict(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7427046",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(train_losses, label='Training loss')\n",
    "ax.plot(valid_losses, label='validuation loss')\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "ax.set_xlabel(r'Epoch')\n",
    "ax.set_ylabel(r'Loss')\n",
    "plt.tight_layout()\n",
    "fig.savefig('../outputs/{}.pdf'.format(model_name), transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffcf364",
   "metadata": {},
   "source": [
    "### Training sample sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da47f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_id = np.random.choice(train_sequence_ids, size=(1,))\n",
    "sample_train_inputs, sample_train_targets = load_data(sample_train_id, recurrent=False)\n",
    "\n",
    "sample_train_inputs = torch.from_numpy(sample_train_inputs)\n",
    "\n",
    "with torch.no_grad():\n",
    "    sample_train_preds = mlp(sample_train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f282def1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.grid()\n",
    "for i in range(6):\n",
    "    ax.plot(sample_train_targets[:, i], color='C{}'.format(i), label='Target')\n",
    "    ax.plot(sample_train_preds[:, i], color='C{}'.format(i), linestyle=':', label='Prediction')\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "handles = handles[:2]\n",
    "labels = labels[:2]\n",
    "ax.legend(handles, labels)\n",
    "ax.set_xlabel('Time step [-]')\n",
    "ax.set_ylabel('Temperature [°C]')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/{}-train.pdf'.format(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82f48ea",
   "metadata": {},
   "source": [
    "### Validation sample sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6416f78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_valid_id = np.random.choice(valid_sequence_ids, size=(1,))\n",
    "sample_valid_inputs, sample_valid_targets = load_data(sample_valid_id, recurrent=False)\n",
    "\n",
    "sample_valid_inputs = torch.from_numpy(sample_valid_inputs)\n",
    "\n",
    "with torch.no_grad():\n",
    "    sample_valid_preds = mlp(sample_valid_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f14b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.grid()\n",
    "for i in range(6):\n",
    "    ax.plot(sample_valid_targets[:, i], color='C{}'.format(i), label='Target')\n",
    "    ax.plot(sample_valid_preds[:, i], color='C{}'.format(i), linestyle=':', label='Prediction')\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "handles = handles[:2]\n",
    "labels = labels[:2]\n",
    "ax.legend(handles, labels)\n",
    "ax.set_xlabel('Time step [-]')\n",
    "ax.set_ylabel('Temperature [°C]')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/{}-valid.pdf'.format(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4efc1b",
   "metadata": {},
   "source": [
    "### Evaluation on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d128c98d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    train_preds = mlp(train_inputs)\n",
    "    train_loss = F.mse_loss(train_preds, train_targets).item()\n",
    "with torch.no_grad():\n",
    "    valid_preds = mlp(valid_inputs)\n",
    "    valid_loss = F.mse_loss(valid_preds, valid_targets).item()\n",
    "with torch.no_grad():\n",
    "    test_preds = mlp(test_inputs)\n",
    "    test_loss = F.mse_loss(test_preds, test_targets).item()\n",
    "\n",
    "print('Train set: {:.4f}'.format(train_loss))\n",
    "print('Validation set: {:.4f}'.format(valid_loss))\n",
    "print('Test set: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9caa13",
   "metadata": {},
   "source": [
    "## Recurent architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef3d183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499814bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "permutation = np.random.permutation(np.arange(1, NUM_SEQUENCES + 1))\n",
    "first_split = int(0.7 * NUM_SEQUENCES)\n",
    "second_split = int(0.85 * NUM_SEQUENCES)\n",
    "train_sequence_ids = permutation[:first_split]\n",
    "valid_sequence_ids = permutation[first_split:second_split]\n",
    "test_sequence_ids = permutation[second_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ed603e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, train_targets = load_data(train_sequence_ids, recurrent=True)\n",
    "valid_inputs, valid_targets = load_data(valid_sequence_ids, recurrent=True)\n",
    "test_inputs, test_targets = load_data(test_sequence_ids, recurrent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9759a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = torch.from_numpy(train_inputs)\n",
    "train_targets = torch.from_numpy(train_targets)\n",
    "valid_inputs = torch.from_numpy(valid_inputs)\n",
    "valid_targets = torch.from_numpy(valid_targets)\n",
    "test_inputs = torch.from_numpy(test_inputs)\n",
    "test_targets = torch.from_numpy(test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd036ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, cell, input_size, hidden_size, output_size, num_layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        if cell == 'gru':\n",
    "            self.rnn = nn.GRU(\n",
    "                input_size=input_size,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers)\n",
    "        elif cell == 'lstm':\n",
    "            self.rnn = nn.LSTM(\n",
    "                input_size=input_size,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, h0=None):\n",
    "        x, hn = self.rnn(x, h0)\n",
    "        return self.sequential(x), hn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f065ac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = train_inputs.size(0)\n",
    "\n",
    "num_train = train_inputs.size(1)\n",
    "num_valid = valid_inputs.size(1)\n",
    "num_test = test_inputs.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64906a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_LAYERS = 1\n",
    "LEARNING_RATE = 1e-2\n",
    "NUM_EPOCH_CONVERGENCE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e0bd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(\n",
    "    cell='gru',\n",
    "    input_size=train_inputs.size(2),\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    output_size=train_targets.size(2),\n",
    "    num_layers=NUM_LAYERS)\n",
    "opt = optim.Adam(rnn.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "model_name = 'rnn-' + '-'.join(str(HIDDEN_SIZE) for _ in range(NUM_LAYERS))\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a614827",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "lowest_loss, num_epoch_no_improvement = float('inf'), 0\n",
    "best_weights = deepcopy(rnn.state_dict())\n",
    "train_losses, valid_losses = [], []\n",
    "train_after_epoch_losses = []  # TODO: delete this\n",
    "\n",
    "epoch = 0\n",
    "while num_epoch_no_improvement <= NUM_EPOCH_CONVERGENCE:\n",
    "    \n",
    "    # Training\n",
    "    permutation = torch.randperm(num_train)\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for i in range(0, num_train, BATCH_SIZE):\n",
    "        indices = permutation[i:i+BATCH_SIZE]\n",
    "        batch_inputs = train_inputs[:, indices, :]\n",
    "        batch_targets = train_targets[:, indices, :]\n",
    "        \n",
    "        batch_preds, _ = rnn(batch_inputs)\n",
    "        loss = F.mse_loss(batch_preds, batch_targets)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= int(num_train / BATCH_SIZE)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    epoch += 1\n",
    "    \n",
    "    # Training evaluation after epoch\n",
    "    with torch.no_grad():\n",
    "        train_preds, _ = rnn(train_inputs)\n",
    "        train_after_epoch_loss = F.mse_loss(train_preds, train_targets).item()\n",
    "    \n",
    "    train_after_epoch_losses.append(train_after_epoch_loss)\n",
    "    \n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        valid_preds, _ = rnn(valid_inputs)\n",
    "        valid_loss = F.mse_loss(valid_preds, valid_targets).item()\n",
    "    \n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    if valid_loss < lowest_loss:\n",
    "        lowest_loss = valid_loss\n",
    "        num_epoch_no_improvement = 0\n",
    "        best_weights = deepcopy(rnn.state_dict())\n",
    "    else:\n",
    "        num_epoch_no_improvement += 1\n",
    "    \n",
    "    print('Epoch {:03d}: train: {:.4f}, train after epoch: {:.4f}, valid: {:.4f}'.format(epoch, train_loss, train_after_epoch_loss, valid_loss))\n",
    "\n",
    "rnn.load_state_dict(best_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd081ebb",
   "metadata": {},
   "source": [
    "### Training sample sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a126c9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_id = np.random.choice(train_sequence_ids, size=(1,))\n",
    "sample_train_inputs, sample_train_targets = load_data(sample_train_id, recurrent=True)\n",
    "\n",
    "sample_train_inputs = torch.from_numpy(sample_train_inputs)\n",
    "\n",
    "with torch.no_grad():\n",
    "    sample_train_preds, _ = rnn(sample_train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7382f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(sample_train_preds[:, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c716b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.grid()\n",
    "for i in range(6):\n",
    "    ax.plot(sample_train_targets[:, 0, i], color='C{}'.format(i), label='Target')\n",
    "    ax.plot(sample_train_preds[:, 0, i], color='C{}'.format(i), linestyle=':', label='Prediction')\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "handles = handles[:2]\n",
    "labels = labels[:2]\n",
    "ax.legend(handles, labels)\n",
    "ax.set_xlabel('Time step [-]')\n",
    "ax.set_ylabel('Temperature [°C]')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/{}-train.pdf'.format(model_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
